import numpy as np
import torch as th
import torch.nn.functional as F
import torch_scatter as th_s

from misc_utils import EPS


def bin_func(mzs, ints, mz_max, mz_bin_res, ints_thresh, return_index):
    """
    Bin mass spectrometry data into discrete bins
    
    Parameters:
        mzs: m/z values
        ints: intensity values
        mz_max: maximum m/z value
        mz_bin_res: bin resolution
        ints_thresh: intensity threshold
        return_index: whether to return bin indices
    
    Returns:
        bin_idx or bin_spec: bin indices or binned spectrum
    """
    mzs = np.array(mzs, dtype=np.float32)
    # bins array is generated by np.arange function, starting from mz_bin_res, ending at mz_max + mz_bin_res, with step size mz_bin_res
    # This array represents the bin boundaries for m/z values in the spectrum data
    bins = np.arange(
        mz_bin_res,
        mz_max + mz_bin_res,
        step=mz_bin_res).astype(np.float32)
    bin_idx = np.searchsorted(bins, mzs, side="right")  # Find insertion point for each element in mzs within bins, i.e., bin index for each element
    if return_index:
        return bin_idx.tolist()
    else:
        ints = np.array(ints, dtype=np.float32)
        bin_spec = np.zeros([len(bins)], dtype=np.float32)
        for i in range(len(mzs)):
            if bin_idx[i] < len(bin_spec) and ints[i] >= ints_thresh:
                bin_spec[bin_idx[i]] = max(bin_spec[bin_idx[i]], ints[i])  # If multiple values in same bin, take maximum intensity
        if np.all(bin_spec == 0.):
            print("> warning: bin_spec is all zeros!")
            bin_spec[-1] = 1.
        return bin_spec  # bin_spec is a vector where position represents mass and corresponding value is intensity


def unprocess_spec(spec, transform):
    """
    This function performs inverse normalization or inverse transformation on spectrum (spec) data,
    restoring it from model output normalized or transformed values to approximate range of original spectrum values
    """
    # transform signal
    if transform == "log10":
        max_ints = float(np.log10(1000. + 1.))
        def untransform_fn(x): return 10**x - 1.
    elif transform == "log10over3":
        max_ints = float(np.log10(1000. + 1.) / 3.)
        def untransform_fn(x): return 10**(3 * x) - 1.
    elif transform == "loge":
        max_ints = float(np.log(1000. + 1.))
        def untransform_fn(x): return th.exp(x) - 1.
    elif transform == "sqrt":
        max_ints = float(np.sqrt(1000.))
        def untransform_fn(x): return x**2
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        max_ints = 1000.
        def untransform_fn(x): return x
    else:
        raise ValueError("invalid transform")
    spec = spec / (th.max(spec, dim=-1, keepdim=True)[0] + EPS) * max_ints
    spec = untransform_fn(spec)
    spec = th.clamp(spec, min=0.)
    assert not th.isnan(spec).any()
    return spec


def process_spec(spec, transform, normalization, eps=EPS):
    """
    Process spectrum data with transformation and normalization
    
    Parameters:
        spec: spectrum data tensor
        transform: transformation type (log10, sqrt, etc.)
        normalization: normalization type (l1, l2, none)
        eps: small value to avoid division by zero
    
    Returns:
        processed spectrum tensor
    """
    # scale spectrum so that max value is 1000
    spec = spec / (th.max(spec, dim=-1, keepdim=True)[0] + eps) * 1000.
    # transform signal - Spectrum data often has extremely different response intensities at different m/z values, which may lead to huge numerical differences
    # Logarithmic transformation can reduce the dominance of large values, make data more stable, and reduce the impact of outliers
    if transform == "log10":
        spec = th.log10(spec + 1)
    elif transform == "log10over3":
        spec = th.log10(spec + 1) / 3
    elif transform == "log10over4":
        spec = th.log10(spec + 1) / 4
    elif transform == "loge":
        spec = th.log(spec + 1)
    elif transform == "sqrt":
        spec = th.sqrt(spec)
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        pass
    else:
        raise ValueError("invalid transform")
    # normalize
    if normalization == "l1":  # L1 normalization, makes the L1 norm of feature vector (sum of absolute values) equal to 1
        spec = F.normalize(spec, p=1, dim=-1, eps=eps)
    elif normalization == "l2":  # L2 normalization, makes the L2 norm of feature vector (square root of sum of squares) equal to 1
        spec = F.normalize(spec, p=2, dim=-1, eps=eps)
    elif normalization == "none":
        pass
    else:
        raise ValueError("invalid normalization")
    assert not th.isnan(spec).any()
    return spec


def process_spec_old(spec, transform, normalization, ints_thresh):
    """
    Old version of spectrum processing function (deprecated)
    """
    # scale spectrum so that max value is 1000 - Calculate scaling factor: 1000. / np.max(spec), multiply each element by this factor to make max value 1000
    spec = spec * (1000. / np.max(spec))
    # remove noise - Set intensity values below threshold to 0
    spec = spec * (spec > ints_thresh * np.max(spec)).astype(float)
    # transform signal
    if transform == "log10":
        spec = np.log10(spec + 1)
    elif transform == "log10over3":
        spec = np.log10(spec + 1) / 3
    elif transform == "loge":
        spec = np.log(spec + 1)
    elif transform == "sqrt":
        spec = np.sqrt(spec)
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        pass
    else:
        raise ValueError("invalid transform")
    # normalize
    if normalization == "l1":
        spec = spec / np.sum(np.abs(spec))
    elif normalization == "l2":
        spec = spec / np.sqrt(np.sum(spec**2))
    elif normalization == "none":
        pass
    else:
        raise ValueError("invalid spectrum_normalization")
    return spec


def merge_spec(spec, group_id, transform, normalization, *other_ids):
    """
    Merge spectrum data based on group identifiers
    
    Function: Merge spectrum data spec according to group identifier group_id, supporting normalization and transformation,
    while processing additional identifiers (other_ids).

    Parameters:
    - spec: Spectrum data tensor, shape [batch_size, num_bins], representing spectrum features for each sample.
    - group_id: Group identifier tensor, shape [batch_size] or higher dimension, indicating which spectra belong to the same group.
    - transform: Type of transformation applied to spectrum data (such as log transformation, inverse transformation, etc.).
    - normalization: Specifies normalization type for merged spectrum data (such as L1 normalization).
    - *other_ids: Additional identifiers (such as `mol_id`), these identifiers also need to be merged by group.

    Returns:
    - spec_merge: Merged spectrum data tensor.
    - un_group_id: Unique group identifier tensor.
    - other_ids_merge: Merged additional identifiers (if provided).
    """
    un_group_id, un_group_idx = th.unique(group_id, dim=0, return_inverse=True)  # un_group_id: unique group identifier tensor, un_group_idx: group index for each sample
    spec_u = unprocess_spec(spec, transform)  # Restore spectrum data from normalized or transformed space to original space
    # th_s.scatter_mean: Group average spectrum data spec_u on specified dimension (dim=0) according to group index un_group_idx
    # spec_u: restored spectrum data. un_group_idx: group index for each sample, indicating which group it belongs to. dim_size=un_group_id.shape[0]: specify number of groups, i.e., number of unique groups.
    spec_merge_u = th_s.scatter_mean(
        spec_u, un_group_idx, dim=0, dim_size=un_group_id.shape[0])
    spec_merge = process_spec(spec_merge_u, transform, normalization)  # Apply specified transformation (e.g., log transformation) and normalization (e.g., l1 normalization) to merged spectrum data
    other_ids_merge = []
    for other_id in other_ids:
        # assumes that all of the entries in other_id are the same (given batch info)
        # uses max instead of mean to avoid floating point errors
        # th_s.scatter_max: This is a function for aggregation by group. Its function is to calculate maximum value of elements in input tensor (other_id) on specified dimension (dim) according to group index (un_group_idx)
        other_id_merge = th_s.scatter_max(  # scatter_max: aggregate additional identifiers by group using maximum value (avoid floating point errors)
            other_id,
            un_group_idx,
            dim=0,
            dim_size=un_group_id.shape[0])[0].type(
            other_id.dtype)
        other_ids_merge.append(other_id_merge)
    return (spec_merge, un_group_id) + tuple(other_ids_merge)  # Return merged spectrum data, unique group identifiers, and merged additional identifiers


def verify_merge(merge_id, unmerge_vals, merge_vals):
    """
    Verify correctness of merge operation
    
    Parameters:
        merge_id: merge identifier
        unmerge_vals: unmerged values
        merge_vals: merged values
    
    Returns:
        bool: whether merge is correct
    """
    un_merge_id, merge_idx = th.unique(merge_id, return_inverse=True)
    if merge_id.shape[0] != unmerge_vals.shape[0]:
        print("shape 0")
        return False
    if un_merge_id.shape[0] != merge_vals.shape[0]:
        print("shape 1")
        return False
    # verify that the merge is correct
    for i in range(len(merge_idx)):
        if not th.all(unmerge_vals[i] == merge_vals[merge_idx[i]]):
            print(f"vals {i}")
            return False
    return True
