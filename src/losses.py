import numpy as np
import torch as th
import torch.nn.functional as F
import torch_scatter as th_s

from misc_utils import EPS


def bin_func(mzs, ints, mz_max, mz_bin_res, ints_thresh, return_index):

    mzs = np.array(mzs, dtype=np.float32)
    # The bins array is generated by the np.arange function, starting from mz_bin_res, ending at mz_max + mz_bin_res, with step size mz_bin_res. This array represents the bin boundaries for mass-to-charge ratio (m/z) in spectral data
    bins = np.arange(
        mz_bin_res,
        mz_max +
        mz_bin_res,
        step=mz_bin_res).astype(
        np.float32)
    bin_idx = np.searchsorted(bins, mzs, side="right") # This function finds the insertion point for each element in mzs within bins, i.e., the bin index to which each element belongs
    if return_index:
        return bin_idx.tolist()
    else:
        ints = np.array(ints, dtype=np.float32)
        bin_spec = np.zeros([len(bins)], dtype=np.float32)
        for i in range(len(mzs)):
            # if mzs[i] > mz_max:
            #     continue  # Skip points exceeding mz_max
            if bin_idx[i] < len(bin_spec) and ints[i] >= ints_thresh:
                bin_spec[bin_idx[i]] = max(bin_spec[bin_idx[i]], ints[i]) # If multiple bins are in the same position, take the one with maximum abundance
        if np.all(bin_spec == 0.):
            print("> warning: bin_spec is all zeros!")
            bin_spec[-1] = 1.
        return bin_spec # bin_spec is a vector where the position represents mass and the corresponding value represents abundance


def unprocess_spec(spec, transform):
    # This function is used to perform inverse normalization or inverse transformation on spectral (spec) data, restoring it from normalized or transformed values of model output to the approximate range of original spectral values
    # transform signal
    if transform == "log10":
        max_ints = float(np.log10(1000. + 1.))
        def untransform_fn(x): return 10**x - 1.
    elif transform == "log10over3":
        max_ints = float(np.log10(1000. + 1.) / 3.)
        def untransform_fn(x): return 10**(3 * x) - 1.
    elif transform == "loge":
        max_ints = float(np.log(1000. + 1.))
        def untransform_fn(x): return th.exp(x) - 1.
    elif transform == "sqrt":
        max_ints = float(np.sqrt(1000.))
        def untransform_fn(x): return x**2
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        max_ints = 1000.
        def untransform_fn(x): return x
    else:
        raise ValueError("invalid transform")
    spec = spec / (th.max(spec, dim=-1, keepdim=True)[0] + EPS) * max_ints
    spec = untransform_fn(spec)
    spec = th.clamp(spec, min=0.)
    assert not th.isnan(spec).any()
    return spec


def process_spec(spec, transform, normalization, eps=EPS):

    # scale spectrum so that max value is 1000
    spec = spec / (th.max(spec, dim=-1, keepdim=True)[0] + eps) * 1000.
    # transform signal Spectral data often has extremely different response intensities at different mass-to-charge ratios (m/z), which may lead to huge numerical differences. Logarithmic transformation can reduce the dominance of large values on results, making data more stable and reducing the impact of outliers
    if transform == "log10":
        spec = th.log10(spec + 1)
    elif transform == "log10over3":
        spec = th.log10(spec + 1) / 3
    elif transform == "log10over4":
        spec = th.log10(spec + 1) / 4
    elif transform == "loge":
        spec = th.log(spec + 1)
    elif transform == "sqrt":
        spec = th.sqrt(spec)
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        pass
    else:
        raise ValueError("invalid transform")
    # normalize
    if normalization == "l1": # L1 normalization, making the L1 norm of the feature vector (i.e., sum of absolute values of all elements) equal to 1
        spec = F.normalize(spec, p=1, dim=-1, eps=eps)
    elif normalization == "l2": # L2 normalization, making the L2 norm of the feature vector (i.e., square root of sum of squares of all elements) equal to 1
        spec = F.normalize(spec, p=2, dim=-1, eps=eps)
    elif normalization == "none":
        pass
    else:
        raise ValueError("invalid normalization")
    assert not th.isnan(spec).any()
    return spec


def process_spec_old(spec, transform, normalization, ints_thresh):

    # scale spectrum so that max value is 1000 Calculate scaling factor: 1000. / np.max(spec), multiply each element in spectral data by this scaling factor to make the maximum value of spectral data become 1000
    spec = spec * (1000. / np.max(spec))
    # remove noise Set intensity values below threshold to 0
    spec = spec * (spec > ints_thresh * np.max(spec)).astype(float)
    # transform signal
    if transform == "log10":
        spec = np.log10(spec + 1)
    elif transform == "log10over3":
        spec = np.log10(spec + 1) / 3
    elif transform == "loge":
        spec = np.log(spec + 1)
    elif transform == "sqrt":
        spec = np.sqrt(spec)
    elif transform == "linear":
        raise NotImplementedError
    elif transform == "none":
        pass
    else:
        raise ValueError("invalid transform")
    # normalize
    if normalization == "l1":
        spec = spec / np.sum(np.abs(spec))
    elif normalization == "l2":
        spec = spec / np.sqrt(np.sum(spec**2))
    elif normalization == "none":
        pass
    else:
        raise ValueError("invalid spectrum_normalization")
    return spec


def merge_spec(spec, group_id, transform, normalization, *other_ids):
    """
    Function: Merge spectral data spec according to grouping identifier group_id, supporting normalization and transformation, while processing additional identifiers (other_ids).

    Parameters:
    - spec: Spectral data tensor with shape [batch_size, num_bins], representing spectral features of each sample.
    - group_id: Grouping identifier tensor with shape [batch_size] or higher dimension, indicating which spectra belong to the same group.
    - transform: Type of transformation applied to spectral data (such as logarithmic transformation, inverse transformation, etc.).
    - normalization: Specifies the normalization type for merged spectral data (such as L1 normalization).
    - *other_ids: Additional identifiers (such as `mol_id`), which also need to be merged by group.

    Return values:
    - spec_merge: Merged spectral data tensor.
    - un_group_id: Unique grouping identifier tensor.
    - other_ids_merge: Merged additional identifiers (if provided).
    """
    un_group_id, un_group_idx = th.unique(group_id, dim=0, return_inverse=True) # un_group_id: Unique grouping identifier tensor, un_group_idx: Group index of each sample
    spec_u = unprocess_spec(spec, transform) # Restore spectral data from normalized or transformed space to original space
    # th_s.scatter_mean: According to grouping index un_group_idx, perform grouped averaging on spectral data spec_u in specified dimension (dim=0)
    # spec_u: Restored spectral data. un_group_idx: Group index of each sample, indicating which group it belongs to. dim_size=un_group_id.shape[0]: Specifies the number of groups, i.e., the number of unique groups
    spec_merge_u = th_s.scatter_mean(
        spec_u, un_group_idx, dim=0, dim_size=un_group_id.shape[0])
    spec_merge = process_spec(spec_merge_u, transform, normalization) # Apply specified transformation (e.g., logarithmic transformation) and normalization (e.g., l1 normalization) to merged spectral data
    other_ids_merge = []
    for other_id in other_ids:
        # assumes that all of the entries in other_id are the same (given batch info)
        # uses max instead of mean to avoid floating point errors       
        # th_s.scatter_max: This is a function that performs aggregation by group. Its function is to calculate the maximum value of elements in the input tensor (other_id) in the specified dimension (dim) according to the grouping index (un_group_idx)
        other_id_merge = th_s.scatter_max( # scatter_max: Perform maximum value aggregation on additional identifiers by group (to avoid floating point errors)
            other_id,
            un_group_idx,
            dim=0,
            dim_size=un_group_id.shape[0])[0].type(
            other_id.dtype)
        other_ids_merge.append(other_id_merge)
    return (spec_merge, un_group_id) + tuple(other_ids_merge) # Return merged spectral data, unique grouping identifiers, and merged additional identifiers


def verify_merge(merge_id,unmerge_vals,merge_vals):

    un_merge_id, merge_idx = th.unique(merge_id, return_inverse=True)
    if merge_id.shape[0] != unmerge_vals.shape[0]:
        print("shape 0")
        return False
    if un_merge_id.shape[0] != merge_vals.shape[0]:
        print("shape 1")
        return False
    # verify that the merge is correct
    for i in range(len(merge_idx)):
        if not th.all(unmerge_vals[i] == merge_vals[merge_idx[i]]):
            print(f"vals {i}")
            return False
    return True
